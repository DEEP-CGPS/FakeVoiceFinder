{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8535b832",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Table of Contents](#table-of-contents)\n",
    "- [1) Environment & Imports](#environment-imports)\n",
    "- [2) Experiment Configuration](#experiment-configuration)\n",
    "- [3) Decide Clip Length (Optional)](#decide-clip-length-optional)\n",
    "- [4) Create Experiment Layout](#create-experiment-layout)\n",
    "- [5) Prepare Dataset (Load / Split / Save Originals)](#prepare-dataset-load-split-save-originals)\n",
    "- [6) Prepare Models](#prepare-models)\n",
    "- [7) Sanity Checks](#sanity-checks)\n",
    "- [8) Train & Evaluate (Optional)](#train-evaluate-optional)\n",
    "- [9) Metrics & Plots (Optional)](#metrics-plots-optional)\n",
    "- [Appendix ‚Äî Tips & Troubleshooting](#appendix-tips-troubleshooting) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a78941",
   "metadata": {},
   "source": [
    "<a id=\"basic-example\"></a>\r\n",
    "# Basic Example\r\n",
    "\r\n",
    "This notebook prepares data from the ZIPs (`real.zip`, `fake.zip`), loads **benchmark** models from torchvision (VGG, ResNet, ALEXNET) according to your configuration, and trains for **3 epochs** per transformation.\r\n",
    "\r\n",
    "**Prerequisites**:\r\n",
    "- Notebook folder: `notebooks/`\r\n",
    "- ZIPs in `../dataset/` (at the repo root): `real.zip`, `fake.zip`\r\n",
    "- (Optional) user-provided TorchScript models in `../models/`\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c34eaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to sys.path: D:\\FakeVoiceFinder\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "lib_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if lib_path not in sys.path:\n",
    "    sys.path.insert(0, lib_path)\n",
    "print(\"Project root added to sys.path:\", lib_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257f138a",
   "metadata": {},
   "source": [
    "<a id=\"environment-imports\"></a>\n",
    "## 1) Environment & Imports\n",
    "<a id='sec1'></a>\n",
    "\n",
    "**Goal:** Make sure the environment has the required packages and import the project modules.\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3.9+ (recommended)\n",
    "- `numpy`, `pandas`, `matplotlib`\n",
    "- `librosa`, `soundfile`, `PyWavelets`\n",
    "- `scikit-learn`, `torch`, `torchvision`\n",
    "\n",
    "**Install (example):**\n",
    "```bash\n",
    "pip install numpy pandas matplotlib librosa soundfile PyWavelets scikit-learn torch torchvision\n",
    "```\n",
    "Run the next cell(s) to import `ExperimentConfig`, `CreateExperiment`, and helpers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27312e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Imports principales\n",
    "from pprint import pprint\n",
    "from fakevoicefinder import ExperimentConfig, CreateExperiment, ModelLoader, Trainer, ConfigError,shortest_audio_seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ba8ddc",
   "metadata": {},
   "source": [
    "<a id=\"experiment-configuration\"></a>\n",
    "## 2) Experiment Configuration\n",
    "<a id='sec2'></a>\n",
    "\n",
    "**Goal:** Define models, transforms, and hyperparameters.\n",
    "\n",
    "**Key fields in `ExperimentConfig`:**\n",
    "- `models_list`: e.g., `['alexnet', 'resnet18', 'convnext_tiny']`\n",
    "- `transform_list`: any of `['mel', 'log', 'dwt']`\n",
    "- `mel_params`, `log_params`, `dwt_params`: optional **overrides** (dicts). Defaults are used if not set.\n",
    "- `clip_seconds`: window length (seconds) for each audio (pad/trim). Default is 3.0 s.\n",
    "- `image_size`: **optional** resize for MEL/LOG (e.g., `224` for ViT).\n",
    "\n",
    "Tip: Keep defaults first; only override if you need a different setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "526d958d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config validation ‚úÖ\n",
      "ExperimentConfig:\n",
      "  batch_size     : 8\n",
      "  cache_features : True\n",
      "  clip_seconds   : None\n",
      "  cqt_params     : {'hop_length': 256, 'n_bins': 96, 'bins_per_octave': 24, 'scale': True}\n",
      "  data_path      : ../dataset\n",
      "  device         : gpu\n",
      "  dwt_params     : {'wavelet': 'db6', 'level': 5, 'mode': 'symmetric'}\n",
      "  epochs         : 1\n",
      "  eval_metric    : ['accuracy', 'F1']\n",
      "  fake_zip       : fake.zip\n",
      "  flag_train     : True\n",
      "  image_size     : 224\n",
      "  input_channels : 1\n",
      "  learning_rate  : 0.0001\n",
      "  log_params     : {'n_fft': 2048, 'hop_length': 256}\n",
      "  mel_params     : {'n_mels': 68, 'n_fft': 2048, 'hop_length': 512}\n",
      "  models_list    : ['alexnet']\n",
      "  models_path    : ../models\n",
      "  num_workers    : 4\n",
      "  optimizer      : Adam\n",
      "  outputs_path   : outputs\n",
      "  patience       : 5\n",
      "  real_zip       : real.zip\n",
      "  run_name       : exp_newv5\n",
      "  save_best_only : True\n",
      "  save_models    : True\n",
      "  seed           : 23\n",
      "  transform_list : ['mel', 'cqt']\n",
      "  type_train     : both\n"
     ]
    }
   ],
   "source": [
    "from fakevoicefinder import ExperimentConfig, ConfigError\n",
    "\n",
    "cfg = ExperimentConfig()\n",
    "\n",
    "# Experiment name (folder under outputs/)\n",
    "cfg.run_name = \"exp_newv5\"\n",
    "\n",
    "# Paths (repo-relative)\n",
    "cfg.data_path = \"../dataset\"   # where real.zip and fake.zip are\n",
    "cfg.models_path = \"../models\"  # user TorchScript models\n",
    "\n",
    "# Transforms to generate\n",
    "cfg.transform_list = [\"mel\",\"dwt\",\"log\",\"cqt\"]   # now we use MEL + CQT\n",
    "\n",
    "# --- mel: valid keys ---\n",
    "# n_mels, n_fft, hop_length, win_length, fmin, fmax\n",
    "cfg.mel_params = {\n",
    "    \"n_mels\": 68,\n",
    "    \"n_fft\": 2048,\n",
    "    \"hop_length\": 512,\n",
    "    # \"win_length\": None,\n",
    "    # \"fmin\": 0,\n",
    "    # \"fmax\": None,\n",
    "}\n",
    "\n",
    "# --- log: valid keys ---\n",
    "# n_fft, hop_length, win_length\n",
    "cfg.log_params = {\n",
    "    \"n_fft\": 2048,\n",
    "    \"hop_length\": 256,\n",
    "    # \"win_length\": None,\n",
    "}\n",
    "\n",
    "# --- dwt: valid keys ---\n",
    "# wavelet, level, mode\n",
    "cfg.dwt_params = {\n",
    "    \"wavelet\": \"db6\",\n",
    "    \"level\": 5,\n",
    "    \"mode\": \"symmetric\",\n",
    "}\n",
    "\n",
    "# --- cqt: valid keys ---\n",
    "# hop_length, n_bins, bins_per_octave, fmin, scale\n",
    "cfg.cqt_params = {\n",
    "    \"hop_length\": 256,      # good time‚Äìfrequency tradeoff\n",
    "    \"n_bins\": 96,           # recommended range ~84‚Äì120\n",
    "    \"bins_per_octave\": 24,  # 12 or 24 ‚Üí 24 gives more detail on formants\n",
    "    \"scale\": True,          # more stable spectral distribution\n",
    "    # \"fmin\": 32.70319566,  # C1 (this is the default in the code)\n",
    "    # \"fmin\": 65.40639133,  # C2 if you want to shift focus to vocal range\n",
    "}\n",
    "\n",
    "cfg.image_size = 224\n",
    "\n",
    "# Benchmark models to test\n",
    "cfg.models_list = [\n",
    "    \"alexnet\",\"resnet18\",\"vgg16\",\"vit_b_16\",\"convnext_tiny\"\n",
    "]\n",
    "\n",
    "# Quick smoke-test training\n",
    "cfg.type_train = \"both\"   # 'scratch' | 'pretrain' | 'both'\n",
    "cfg.epochs = 3\n",
    "cfg.batch_size = 8\n",
    "cfg.learning_rate = 0.0001\n",
    "cfg.patience = 5\n",
    "\n",
    "# Input channels for spectrograms (.npy): 1 channel\n",
    "cfg.input_channels = 1 \n",
    "\n",
    "# Config validation\n",
    "try:\n",
    "    cfg.validate()\n",
    "    print(\"Config validation ‚úÖ\")\n",
    "except ConfigError as e:\n",
    "    print(\"Config validation error:\", e)\n",
    "    raise\n",
    "\n",
    "print(cfg.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee627fa",
   "metadata": {},
   "source": [
    "<a id=\"decide-clip-length-optional\"></a>\n",
    "## 3) Decide Clip Length (Optional)\n",
    "<a id='sec3'></a>\n",
    "\n",
    "**Goal:** Choose the time window (`clip_seconds`) to use for all audios.\n",
    "\n",
    "Use `shortest_audio_seconds(cfg)` to scan `reals.zip` and `fakes.zip` and return the shortest duration in seconds. Then either:\n",
    "- **A)** set `cfg.clip_seconds = min_duration` to avoid truncation; or\n",
    "- **B)** choose a fixed value (e.g., 3.0 s). Short files will be **zero-padded** automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b5bd30b-62d1-4b68-a12f-de4382fdf4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duraci√≥n m√≠nima detectada en los zips: 4\n"
     ]
    }
   ],
   "source": [
    "# 2) Elegir la ventana de audio (clip_seconds)\n",
    "min_sec = int(shortest_audio_seconds(cfg))\n",
    "print(f\"Duraci√≥n m√≠nima detectada en los zips: {min_sec}\")\n",
    "\n",
    "# Opci√≥n A: usar exactamente la m√≠nima detectada\n",
    "cfg.clip_seconds = min_sec\n",
    "\n",
    "# Opci√≥n B: usar un valor fijo que t√∫ prefieras (p. ej., 3.0 s)\n",
    "# cfg.clip_seconds = 3.0\n",
    "\n",
    "# Nota: si pones un valor mayor que muchos audios, se rellenar√° con padding (como ya hace el pipeline).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e72ba8",
   "metadata": {},
   "source": [
    "<a id=\"create-experiment-layout\"></a>\n",
    "## 4) Create Experiment Layout\n",
    "<a id='sec4'></a>\n",
    "\n",
    "**Goal:** Initialize the experiment folder structure and manifest (`experiment.json`).\n",
    "\n",
    "Run `CreateExperiment(cfg).build()` to set up:\n",
    "- `outputs/<RUN>/datasets/{train,test}/...`\n",
    "- `outputs/<RUN>/models/loaded/`\n",
    "- `outputs/<RUN>/reports/`\n",
    "\n",
    "The manifest stores paths and metadata for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62fe9e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': {'alexnet': {'loaded_path': None,\n",
       "   'trained_path': None,\n",
       "   'train_parameters': {'epochs': 1,\n",
       "    'learning_rate': 0.0001,\n",
       "    'batch_size': 8,\n",
       "    'optimizer': 'Adam',\n",
       "    'patience': 5,\n",
       "    'device': 'gpu',\n",
       "    'seed': 23,\n",
       "    'type_train': 'both',\n",
       "    'num_workers': 4,\n",
       "    'transform': None}}},\n",
       " 'train_data': {'original_dataset': {'path': 'outputs/exp_newv5/datasets/train/original',\n",
       "   'num_items': 0},\n",
       "  'transforms_dataset': {'mel': {'path': 'outputs/exp_newv5/datasets/train/transforms/mel',\n",
       "    'params': {}},\n",
       "   'cqt': {'path': 'outputs/exp_newv5/datasets/train/transforms/cqt',\n",
       "    'params': {}}}},\n",
       " 'test_data': {'original_dataset': {'path': 'outputs/exp_newv5/datasets/test/original',\n",
       "   'num_items': 0},\n",
       "  'transforms_dataset': {'mel': {'path': 'outputs/exp_newv5/datasets/test/transforms/mel',\n",
       "    'params': {}},\n",
       "   'cqt': {'path': 'outputs/exp_newv5/datasets/test/transforms/cqt',\n",
       "    'params': {}}}},\n",
       " 'reports': {'path': 'outputs/exp_newv5/reports'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = CreateExperiment(cfg, experiment_name=cfg.run_name)\n",
    "exp.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccdd45b",
   "metadata": {},
   "source": [
    "<a id=\"prepare-dataset-load-split-save-originals\"></a>\n",
    "## 5) Prepare Dataset (Load / Split / Save Originals / Transform)\n",
    "<a id='sec5'></a>\n",
    "\n",
    "**Goal:**\n",
    "1) Read `real.zip` and `fake.zip`.\n",
    "2) Stratified split into train/test.\n",
    "3) Extract original audio files into the experiment folders.\n",
    "4) Generate Transforms (MEL / LOG / DWT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b459feb-e6b0-4a35-8f4a-8f6d3e86597d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prep summary:\n",
      "{'load': {'fake': 600, 'real': 600},\n",
      " 'save_original': {'test': 240, 'train': 960},\n",
      " 'split': {'test': {'fake': 120, 'real': 120, 'total': 240},\n",
      "           'train': {'fake': 480, 'real': 480, 'total': 960}},\n",
      " 'transforms': {'cqt': {'test': 240, 'train': 960},\n",
      "                'mel': {'test': 240, 'train': 960}}}\n",
      "Manifest: D:/FakeVoiceFinder/outputs/exp_newv5/experiment.json\n"
     ]
    }
   ],
   "source": [
    "summary = exp.prepare_data(train_ratio=0.8, seed=cfg.seed, transforms=cfg.transform_list)\n",
    "print(\"Data prep summary:\")\n",
    "pprint(summary)\n",
    "\n",
    "print(\"Manifest:\", (exp.root / \"experiment.json\").as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a6d4ee",
   "metadata": {},
   "source": [
    "<a id=\"prepare-models\"></a>\n",
    "## 6) Prepare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48f90718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarks saved under models/loaded:\n",
      "{'alexnet': {'pretrain': 'outputs/exp_newv5/models/loaded/alexnet_pretrain.pt',\n",
      "             'scratch': 'outputs/exp_newv5/models/loaded/alexnet_scratch.pt'}}\n",
      "User models saved:\n",
      "{'SimpleCNN_scripted.pt': 'outputs/exp_newv5/models/loaded/SimpleCNN_scripted_usermodel_jit.pt'}\n"
     ]
    }
   ],
   "source": [
    "loader = ModelLoader(exp)\n",
    "bench = loader.prepare_benchmarks(add_softmax=False, input_channels=getattr(cfg, \"input_channels\", 1))\n",
    "print(\"Benchmarks saved under models/loaded:\")\n",
    "pprint(bench)\n",
    "\n",
    "# User models (if any .pt/.pth under cfg.models_path)\n",
    "user = loader.prepare_user_models(add_softmax=False, input_channels=cfg.input_channels)\n",
    "print(\"User models saved:\")\n",
    "pprint(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d693357-f0a1-478e-a254-9eeab8557dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\FakeVoiceFinder\\outputs\\exp_newv5\\models\\loaded\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(exp.loaded_models)\n",
    "print(exp.loaded_models.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30f077af-fcaa-483e-8348-62db67650f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../models'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.models_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f11848e",
   "metadata": {},
   "source": [
    "<a id=\"sanity-checks\"></a>\n",
    "## 7) Sanity Checks\n",
    "<a id='sec7'></a>\n",
    "\n",
    "**Goal:** Verify shapes and parameters saved to the manifest.\n",
    "\n",
    "- Load one `.npy` per class and print its shape.\n",
    "- Inspect `experiment.json` parameters under `train_data.transforms_dataset[<name>].params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24767786-8c87-4048-bc93-b3efeea47a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ datasets\n",
      "   üìÅ test\n",
      "      üìÅ original\n",
      "         üìÅ fake\n",
      "         üìÅ real\n",
      "      üìÅ transforms\n",
      "         üìÅ cqt\n",
      "         üìÅ mel\n",
      "   üìÅ train\n",
      "      üìÅ original\n",
      "         üìÅ fake\n",
      "         üìÅ real\n",
      "      üìÅ transforms\n",
      "         üìÅ cqt\n",
      "         üìÅ mel\n",
      "üìÅ models\n",
      "   üìÅ loaded\n",
      "      üìÑ alexnet_pretrain.pt\n",
      "      üìÑ alexnet_scratch.pt\n",
      "      üìÑ SimpleCNN_scripted_usermodel_jit.pt\n",
      "   üìÅ trained\n",
      "üìÅ reports\n",
      "üìÑ experiment.json\n"
     ]
    }
   ],
   "source": [
    "def print_tree(root: Path, max_depth: int = 3, prefix: str = \"\"):\n",
    "    if max_depth < 0:\n",
    "        return\n",
    "    try:\n",
    "        entries = sorted(root.iterdir(), key=lambda p: (p.is_file(), p.name.lower()))\n",
    "    except FileNotFoundError:\n",
    "        return\n",
    "    for e in entries:\n",
    "        print(prefix + (\"üìÑ \" if e.is_file() else \"üìÅ \") + e.name)\n",
    "        if e.is_dir():\n",
    "            print_tree(e, max_depth - 1, prefix + \"   \")\n",
    "\n",
    "print_tree(exp.root, max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d3156b",
   "metadata": {},
   "source": [
    "<a id=\"train-evaluate-optional\"></a>\n",
    "## 8) Train & Evaluate (Optional)\n",
    "<a id='sec8'></a>\n",
    "\n",
    "**Goal:** Train your selected models and compute metrics on the test split.\n",
    "\n",
    "- Use your training loop or the provided trainer to fit each model.\n",
    "- Evaluate with `MetricsReporter` to build a DataFrame of scores (Accuracy, F1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9dc1fa9b-0a46-45d2-bce3-2df1bf6f34ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('D:/FakeVoiceFinder/outputs/exp_newv5/models/loaded')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.loaded_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b966a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trainer] Using device: cuda\n",
      "[Trainer] Transforms to train: ['mel', 'cqt']\n",
      "[Trainer] Models found: ['alexnet', 'usermodel_SimpleCNN_scripted.pt']\n",
      "\n",
      "=== MODEL: alexnet ===\n",
      "[alexnet] Hyperparams -> epochs=1, lr=0.0001, bs=8, optimizer=Adam, patience=5, seed=23, num_workers=4\n",
      "[alexnet][mel] Dataset sizes -> train: 1920, test: 480\n",
      "[alexnet][mel] Batches -> train: 240, test: 60\n",
      "[alexnet][mel][scratch] Loading checkpoint: D:\\FakeVoiceFinder\\outputs\\exp_newv5\\models\\loaded\\alexnet_scratch.pt\n",
      "[alexnet][mel][scratch] Loaded pickled module.\n",
      "[alexnet][mel][scratch] Start training for 1 epochs\n",
      "[alexnet][mel][scratch] Epoch 1/1 - loss=0.6964 acc=0.5000\n",
      "[alexnet][mel][scratch] Confusion matrix (test):\n",
      "[[TN= 240, FP=   0],\n",
      " [FN= 240, TP=   0]]\n",
      "[alexnet][mel][scratch] ‚úÖ New best acc=0.5000 at epoch 1\n",
      "[alexnet][mel] Saved best checkpoint -> alexnet_scratch_mel_seed23_epoch001_acc0.50.pt\n",
      "[alexnet][mel][pretrain] Loading checkpoint: D:\\FakeVoiceFinder\\outputs\\exp_newv5\\models\\loaded\\alexnet_pretrain.pt\n",
      "[alexnet][mel][pretrain] Loaded pickled module.\n",
      "[alexnet][mel][pretrain] Start training for 1 epochs\n",
      "[alexnet][mel][pretrain] Epoch 1/1 - loss=0.6150 acc=0.7875\n",
      "[alexnet][mel][pretrain] Confusion matrix (test):\n",
      "[[TN= 142, FP=  98],\n",
      " [FN=   4, TP= 236]]\n",
      "[alexnet][mel][pretrain] ‚úÖ New best acc=0.7875 at epoch 1\n",
      "[alexnet][mel] Saved best checkpoint -> alexnet_pretrain_mel_seed23_epoch001_acc0.79.pt\n",
      "[alexnet][cqt] Dataset sizes -> train: 1920, test: 480\n",
      "[alexnet][cqt] Batches -> train: 240, test: 60\n",
      "[alexnet][cqt][scratch] Loading checkpoint: D:\\FakeVoiceFinder\\outputs\\exp_newv5\\models\\loaded\\alexnet_scratch.pt\n",
      "[alexnet][cqt][scratch] Loaded pickled module.\n",
      "[alexnet][cqt][scratch] Start training for 1 epochs\n",
      "[alexnet][cqt][scratch] Epoch 1/1 - loss=0.6963 acc=0.5000\n",
      "[alexnet][cqt][scratch] Confusion matrix (test):\n",
      "[[TN= 240, FP=   0],\n",
      " [FN= 240, TP=   0]]\n",
      "[alexnet][cqt][scratch] ‚úÖ New best acc=0.5000 at epoch 1\n",
      "[alexnet][cqt] Saved best checkpoint -> alexnet_scratch_cqt_seed23_epoch001_acc0.50.pt\n",
      "[alexnet][cqt][pretrain] Loading checkpoint: D:\\FakeVoiceFinder\\outputs\\exp_newv5\\models\\loaded\\alexnet_pretrain.pt\n",
      "[alexnet][cqt][pretrain] Loaded pickled module.\n",
      "[alexnet][cqt][pretrain] Start training for 1 epochs\n",
      "[alexnet][cqt][pretrain] Epoch 1/1 - loss=0.6010 acc=0.7917\n",
      "[alexnet][cqt][pretrain] Confusion matrix (test):\n",
      "[[TN= 180, FP=  60],\n",
      " [FN=  40, TP= 200]]\n",
      "[alexnet][cqt][pretrain] ‚úÖ New best acc=0.7917 at epoch 1\n",
      "[alexnet][cqt] Saved best checkpoint -> alexnet_pretrain_cqt_seed23_epoch001_acc0.79.pt\n",
      "\n",
      "=== MODEL: usermodel_SimpleCNN_scripted.pt ===\n",
      "[usermodel_SimpleCNN_scripted.pt] Hyperparams -> epochs=1, lr=0.0001, bs=8, optimizer=Adam, patience=5, seed=23, num_workers=4\n",
      "[usermodel_SimpleCNN_scripted.pt][mel] Dataset sizes -> train: 1920, test: 480\n",
      "[usermodel_SimpleCNN_scripted.pt][mel] Batches -> train: 240, test: 60\n",
      "[usermodel_SimpleCNN_scripted.pt][mel][usermodel_jit] Loading checkpoint: D:\\FakeVoiceFinder\\outputs\\exp_newv5\\models\\loaded\\SimpleCNN_scripted_usermodel_jit.pt\n",
      "[usermodel_SimpleCNN_scripted.pt][mel][usermodel_jit] Loaded TorchScript module.\n",
      "[usermodel_SimpleCNN_scripted.pt][mel][usermodel_jit] Start training for 1 epochs\n",
      "[usermodel_SimpleCNN_scripted.pt][mel][usermodel_jit] Epoch 1/1 - loss=0.5997 acc=0.8000\n",
      "[usermodel_SimpleCNN_scripted.pt][mel][usermodel_jit] Confusion matrix (test):\n",
      "[[TN= 214, FP=  26],\n",
      " [FN=  70, TP= 170]]\n",
      "[usermodel_SimpleCNN_scripted.pt][mel][usermodel_jit] ‚úÖ New best acc=0.8000 at epoch 1\n",
      "[usermodel_SimpleCNN_scripted.pt][mel] Saved best checkpoint -> usermodel_SimpleCNN_scripted_pt_usermodel_jit_mel_seed23_epoch001_acc0.80.pt\n",
      "[usermodel_SimpleCNN_scripted.pt][cqt] Dataset sizes -> train: 1920, test: 480\n",
      "[usermodel_SimpleCNN_scripted.pt][cqt] Batches -> train: 240, test: 60\n",
      "[usermodel_SimpleCNN_scripted.pt][cqt][usermodel_jit] Loading checkpoint: D:\\FakeVoiceFinder\\outputs\\exp_newv5\\models\\loaded\\SimpleCNN_scripted_usermodel_jit.pt\n",
      "[usermodel_SimpleCNN_scripted.pt][cqt][usermodel_jit] Loaded TorchScript module.\n",
      "[usermodel_SimpleCNN_scripted.pt][cqt][usermodel_jit] Start training for 1 epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(exp)\n",
    "train_results = trainer.train_all()\n",
    "print(\"Resultados de entrenamiento (rutas repo-relativas):\")\n",
    "pprint(train_results)\n",
    "\n",
    "print(\"Best checkpoints stored in:\", (exp.trained_models).as_posix())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd48b136-29e6-493d-afaf-4e437de92fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(root: Path, max_depth: int = 3, prefix: str = \"\"):\n",
    "    if max_depth < 0:\n",
    "        return\n",
    "    try:\n",
    "        entries = sorted(root.iterdir(), key=lambda p: (p.is_file(), p.name.lower()))\n",
    "    except FileNotFoundError:\n",
    "        return\n",
    "    for e in entries:\n",
    "        print(prefix + (\"üìÑ \" if e.is_file() else \"üìÅ \") + e.name)\n",
    "        if e.is_dir():\n",
    "            print_tree(e, max_depth - 1, prefix + \"   \")\n",
    "\n",
    "print_tree(exp.root, max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee85ade",
   "metadata": {},
   "source": [
    "<a id=\"metrics-plots-optional\"></a>\n",
    "## 9) Metrics & Plots (Optional)\n",
    "<a id='sec9'></a>\n",
    "\n",
    "**Goal:** Visualize results.\n",
    "\n",
    "- `plot_architectures_for_transform`: bar chart of `(model+variant)` for a single transform.\n",
    "- `plot_variants_for_model`: bar chart of `(transform)` for a single `(model, variant)`.\n",
    "- `plot_heatmap_models_transforms`: heatmap over `(models√óvariants) √ó transforms`.\n",
    "\n",
    "**Color map defaults:** worst value ‚Üí red, better ‚Üí green, max at 100%. Each cell shows its value in %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddec955-f81c-4aab-9254-f727dfe9202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fakevoicefinder.config import ExperimentConfig\n",
    "from fakevoicefinder.experiment import CreateExperiment\n",
    "from fakevoicefinder.metrics import MetricsReporter\n",
    "\n",
    "EXP_NAME = cfg.run_name  \n",
    "\n",
    "cfg = ExperimentConfig(); cfg.run_name = EXP_NAME\n",
    "exp = CreateExperiment(cfg, experiment_name=cfg.run_name)  \n",
    "\n",
    "rep = MetricsReporter(exp)                   # toma reports/ del manifest\n",
    "df = rep.evaluate_all(\"metrics_summary.csv\") # guarda CSV en outputs/<exp>/reports/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380c093b-d4e4-4aaa-b2b1-cd379e9fbcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c40aa08-c596-4335-adb4-3302bc0adbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figuras (se guardan en 'reports/' al pasar out_name)\n",
    "rep.plot_architectures_for_transform(df, transform=\"mel\", metric=\"accuracy\",\n",
    "                                     y_min=0, y_max=100, out_name=\"fig_arch_mel_acc.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc0736-51d2-4703-84e1-fc6bd765d3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep.plot_variants_for_model(df, model=\"alexnet\", variant=\"pretrain\", metric=\"accuracy\",\n",
    "                              y_min=0, y_max=100, out_name=\"fig_alexnet_pretrain_accuracy.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fae50da-8b87-41b8-8cdb-be56c9ba0bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep.plot_heatmap_models_transforms(df, metric=\"accuracy\", vmin=50, vmax=100,\n",
    "                                   out_name=\"fig_all.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f7832b",
   "metadata": {},
   "source": [
    "<a id=\"appendix-tips-troubleshooting\"></a>\n",
    "## Appendix ‚Äî Tips & Troubleshooting\n",
    "<a id='sec10'></a>\n",
    "\n",
    "- If using ViT/ConvNeXt, prefer **224√ó224** inputs. Set `cfg.image_size = 224` (MEL/LOG) or use **DWT**.\n",
    "- If audio files vary in length, pick an appropriate `clip_seconds`. Shorter files are zero-padded.\n",
    "- If you change transform hyperparameters, rerun the transform step to regenerate features.\n",
    "- Ensure CUDA is available if `cfg.device='gpu'`. Otherwise, it will fall back to CPU.\n",
    "- Check `outputs/<RUN>/reports/` for figures and CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c338f43-3b64-40c6-ac64-98f73f6d905b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:environmenttestfake]",
   "language": "python",
   "name": "conda-env-environmenttestfake-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
